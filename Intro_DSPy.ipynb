{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/GenAI_Hands_On/blob/main/Intro_DSPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1de941",
      "metadata": {
        "id": "5e1de941"
      },
      "source": [
        "\n",
        "# Introduction to DSPy\n",
        "This interactive notebook is meant as a **learning guide** on how to use DSPy.\n",
        "\n",
        "You will be prompted to fill in key code sections yourself. For each exercise, try to write the code before revealing the solution!\n",
        "\n",
        "---\n",
        "\n",
        "You will learn about:\n",
        "- The role of language models (LMs) / GenAI\n",
        "- How DSPy Signatures and Modules are used\n",
        "\n",
        "In the following sections, we will explore these concepts with code examples and explanations.\n",
        "\n",
        "DSPy is designed for building **modular, explainable, and robust** AI pipelines that combine language models, structured reasoning, and tool integration. For packaging classification, this brings several advantages.\n",
        "\n",
        "The main benefit of DSPy is that it is a **declarative language**. The idea is to take away the manual prompt optimisation part and change it into a requirement formulation/programming task. This approach allows automatic prompt optimization methods.\n",
        "\n",
        "DSPy makes it straightforward to improve your pipelines by:\n",
        "\n",
        "- **Learnable Parameters:** DSPy modules can include learnable parameters (such as prompt templates or tool selection strategies) that can be automatically tuned for better performance.\n",
        "- **Automated Tuning:** DSPy provides optimizers (like `dspy.optimize`) that can automatically search for the best configuration of your pipeline, using labeled data or feedback signals.\n",
        "- **End-to-End Pipeline Optimization:** You can optimize not just individual prompts, but entire pipelines—including tool usage, reasoning steps, and output normalization—without rewriting your code.\n",
        "- **Rapid Experimentation:** Because DSPy modules are composable and declarative, you can quickly try different optimization strategies and compare results, accelerating development cycles.\n",
        "\n",
        "This means you can move from prototyping to production-ready, high-performing pipelines much faster, with less manual prompt engineering and more robust, data-driven improvements. The topic of optimization will be out of scope of this introductory notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7014a4cb",
      "metadata": {
        "id": "7014a4cb"
      },
      "source": [
        "\n",
        "## Environment Setup\n",
        "\n",
        "This workshop uses:\n",
        "- Python 3.10+\n",
        "- `dspy` (or `dspy-ai`) for programmable, optimizable LLM pipelines\n",
        "- `pandas` for data handling\n",
        "- `rapidfuzz` for string similarity\n",
        "- An LLM provider (Gemini or OpenAI or compatible).\n",
        "\n",
        "> If you don't have Internet in your environment, skip installs and read through the code; it will still serve as a template.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your environment allows, uncomment to install.\n",
        "!pip install --quiet dspy-ai rapidfuzz pandas python-dotenv\n",
        "# For Google API:\n",
        "!pip install --quiet google-generativeai"
      ],
      "metadata": {
        "id": "gMoKJUEbudNI"
      },
      "id": "gMoKJUEbudNI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "tCBHGYLiu0Iu"
      },
      "id": "tCBHGYLiu0Iu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete. Data directory:\", DATA_DIR.resolve())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOGXkZhu51I",
        "outputId": "49198753-e67c-4277-a762-507747a8119b"
      },
      "id": "PXOGXkZhu51I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Data directory: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee5191a",
      "metadata": {
        "id": "3ee5191a"
      },
      "source": [
        "\n",
        "### 0.1) Configure Model / API Keys\n",
        "\n",
        "You can use **Google Gemini** or **OpenAI** or any provider supported by DSPy\n",
        "Set the env var(s) appropriately, then initialize the DSPy model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892d62ab",
      "metadata": {
        "id": "892d62ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dspy\n",
        "\n",
        "# Configure API Key\n",
        "GEMINI_API_KEY = \"AIzaSyAh0Kp5YuOCTmc5qKNo0R5cWzWWGe8x_OQ\"\n",
        "GEMINI_MODEL = \"gemini/gemini-2.5-flash\" # gemini-2.0-flash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa415e52",
      "metadata": {
        "id": "fa415e52"
      },
      "source": [
        "\n",
        "### 0.2) Initialize DSPy with your chosen language model\n",
        "\n",
        "Let's set up a simple LLM in DSPy. **Fill in the code cell below to:**\n",
        "- Import dspy\n",
        "- Set up a language model (you can use a placeholder for the API key)\n",
        "- Configure dspy to use this LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30d9b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a30d9b01",
        "outputId": "5b713d7d-bce7-44e3-bd2c-3c16b24c2395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy initialized with model: gemini/gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "# If DSPy is installed, this will work. Otherwise, treat as reference code.\n",
        "try:\n",
        "    import dspy\n",
        "    # Initialize a Gemini-based LM for DSPy (e.g., Gemini-2.5-flash)\n",
        "    llm = dspy.LM(\n",
        "        model= GEMINI_MODEL,\n",
        "        api_key=GEMINI_API_KEY\n",
        "    )\n",
        "    dspy.settings.configure(lm=llm)\n",
        "    print(\"DSPy initialized with model:\", GEMINI_MODEL)\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available or failed to initialize:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Calling the LLM\n",
        "\n",
        "Write a code cell to call the LLM with a simple prompt, e.g., 'Say this is a test!'.\n",
        "\n",
        "Note! If this does not work, most likely something is wrong with the setup of your LLM."
      ],
      "metadata": {
        "id": "t-jLqUtWhBSu"
      },
      "id": "t-jLqUtWhBSu"
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"Say: this is a test!\", temperature=0.7)  # => ['This is a test!']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq2FVJ-lg6fA",
        "outputId": "0ace29dd-e088-4e2c-8c2e-9bb28099c8d3"
      },
      "id": "cq2FVJ-lg6fA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a test!']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the traditional role format: messages=\n",
        "[{\"role\": \"user\", \"content\": \"Say this is not a test!\"}]\n",
        "Try it here."
      ],
      "metadata": {
        "id": "i2mlWdwihZdv"
      },
      "id": "i2mlWdwihZdv"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Call the LLM with the messages format"
      ],
      "metadata": {
        "id": "hd4FIxKShc7x"
      },
      "id": "hd4FIxKShc7x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "llm(messages=[{\"role\": \"user\", \"content\": \"Say this is not a test!\"}])  # => ['This is not a test!']\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "qujfzRI9hgkx"
      },
      "id": "qujfzRI9hgkx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSPy Signatures and Modules\n",
        "\n",
        "**Exercise:** Define a simple DSPy signature for sentiment classification.\n",
        "\n",
        "- Create a class `Classify` inheriting from `dspy.Signature`\n",
        "- Add input and output fields for sentence, sentiment, and confidence\n",
        "- Instantiate a Predict module and use it on a sample sentence"
      ],
      "metadata": {
        "id": "coTovYW-h74_"
      },
      "id": "coTovYW-h74_"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "class Classify(dspy.Signature):\n",
        "    \"\"\"Classify sentiment of a given sentence.\"\"\"\n",
        "\n",
        "    sentence: str = dspy.InputField()\n",
        "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = dspy.OutputField()\n",
        "    confidence: float = dspy.OutputField()\n",
        "\n",
        "classify = dspy.Predict(Classify)\n",
        "classify(sentence=\"This book was super fun to read, though not the last chapter.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQEI8q9GiAVz",
        "outputId": "6d47fdee-256e-4f20-defc-119d9eaa8254"
      },
      "id": "bQEI8q9GiAVz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    sentiment='positive',\n",
              "    confidence=0.75\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBlBbevXin_B",
        "outputId": "80e8fda2-7d6e-492e-a4e2-450df5bb2bd5"
      },
      "id": "dBlBbevXin_B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': None,\n",
              "  'messages': [{'role': 'system',\n",
              "    'content': \"Your input fields are:\\n1. `sentence` (str):\\nYour output fields are:\\n1. `sentiment` (Literal['positive', 'negative', 'neutral']): \\n2. `confidence` (float):\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## sentence ## ]]\\n{sentence}\\n\\n[[ ## sentiment ## ]]\\n{sentiment}        # note: the value you produce must exactly match (no extra characters) one of: positive; negative; neutral\\n\\n[[ ## confidence ## ]]\\n{confidence}        # note: the value you produce must be a single float value\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Classify sentiment of a given sentence.\"},\n",
              "   {'role': 'user',\n",
              "    'content': \"[[ ## sentence ## ]]\\nThis book was super fun to read, though not the last chapter.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## sentiment ## ]]` (must be formatted as a valid Python Literal['positive', 'negative', 'neutral']), then `[[ ## confidence ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
              "  'kwargs': {},\n",
              "  'response': ModelResponse(id='Am2saLbkJ7Cc-8YPw6CtwAs', created=1756130558, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## sentiment ## ]]\\npositive\\n\\n[[ ## confidence ## ]]\\n0.75\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage={}, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[], cache_hit=None),\n",
              "  'outputs': ['[[ ## sentiment ## ]]\\npositive\\n\\n[[ ## confidence ## ]]\\n0.75\\n\\n[[ ## completed ## ]]'],\n",
              "  'usage': {},\n",
              "  'cost': 0.0013453,\n",
              "  'timestamp': '2025-08-25T14:56:29.123021',\n",
              "  'uuid': '70444461-b3d4-4744-8d61-1720e129020a',\n",
              "  'model': 'gemini/gemini-2.5-flash',\n",
              "  'response_model': 'gemini-2.5-flash',\n",
              "  'model_type': 'chat'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.inspect_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MRql2tpief4",
        "outputId": "1cdb0aad-960f-4839-ea18-9c5d5b94d0b7"
      },
      "id": "_MRql2tpief4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-08-25T14:56:29.123021]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `sentence` (str):\n",
            "Your output fields are:\n",
            "1. `sentiment` (Literal['positive', 'negative', 'neutral']): \n",
            "2. `confidence` (float):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## sentence ## ]]\n",
            "{sentence}\n",
            "\n",
            "[[ ## sentiment ## ]]\n",
            "{sentiment}        # note: the value you produce must exactly match (no extra characters) one of: positive; negative; neutral\n",
            "\n",
            "[[ ## confidence ## ]]\n",
            "{confidence}        # note: the value you produce must be a single float value\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Classify sentiment of a given sentence.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## sentence ## ]]\n",
            "This book was super fun to read, though not the last chapter.\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## sentiment ## ]]` (must be formatted as a valid Python Literal['positive', 'negative', 'neutral']), then `[[ ## confidence ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## sentiment ## ]]\n",
            "positive\n",
            "\n",
            "[[ ## confidence ## ]]\n",
            "0.75\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd43331",
      "metadata": {
        "id": "5dd43331"
      },
      "source": [
        "\n",
        "---\n",
        "## Warm-Up (GenAI Basics): Product Description Generator (FMCG)\n",
        "\n",
        "**Goal:** See how style, tone, and temperature affect outputs.\n",
        "\n",
        "**Task:** Given a product name & features, generate a short marketing description in 3 tones.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Different Tones\n",
        "product = \"Sunburst Orange Juice\"\n",
        "tones = [\"Formal\", \"Casual\", \"Punchy / Ad-like\"]"
      ],
      "metadata": {
        "id": "2mby186jxOkA"
      },
      "id": "2mby186jxOkA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: write a DSPy predict function that has input of tone and product and outputs product description"
      ],
      "metadata": {
        "id": "OTbJ9U4Bj7jP"
      },
      "id": "OTbJ9U4Bj7jP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "# ✅ Define signature\n",
        "class ProductDescription(dspy.Signature):\n",
        "    \"\"\"Generate a product description given tone and product.\"\"\"\n",
        "    tone = dspy.InputField()\n",
        "    product = dspy.InputField()\n",
        "    description = dspy.OutputField()\n",
        "\n",
        "# Predict module\n",
        "gen = dspy.Predict(ProductDescription)\n",
        "\n",
        "for tone in tones:\n",
        "    result = gen(tone=tone, product=product)\n",
        "    print(f\"\\n--- {tone} ---\\n{result.description}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "0GXsaLYhj6zt"
      },
      "id": "0GXsaLYhj6zt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***No writing prompts***"
      ],
      "metadata": {
        "id": "Tz-ZS2dL0QA4"
      },
      "id": "Tz-ZS2dL0QA4"
    },
    {
      "cell_type": "code",
      "source": [
        "# gen.inspect_history()"
      ],
      "metadata": {
        "id": "ciLCW4m70TYE"
      },
      "id": "ciLCW4m70TYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of DSPy Modules: Predict, ChainOfThought, and ReAct\n",
        "\n",
        "DSPy provides several module types to structure and control how language models solve tasks.\n",
        "What do you think Predict, ChainofThought and ReAct are used for?\n",
        "\n",
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "**dspy.Predict**:\n",
        "- The simplest module, used for direct prediction tasks.\n",
        "- Takes a signature and uses the LM to generate outputs based on the defined input/output fields.\n",
        "- Example: Sentiment classification, Product Description.\n",
        "\n",
        "**dspy.ChainOfThought**:\n",
        "- Encourages the LM to reason step-by-step before a final answer.\n",
        "- Useful for tasks that benefit from intermediate reasoning, such as complex classification or multi-step decision making.\n",
        "- Example: Explaining why a packaging type was chosen before giving the answer.\n",
        "\n",
        "**dspy.ReAct**:\n",
        "- This is essentially a react agent, that can interact with tools or external APIs during its reasoning process.\n",
        "- Useful for tasks that require both thought and action, such as searching for information or calling functions.\n",
        "- Example: The LM may search for product images, then download and view them, before deciding whether the image is appropriate.\n",
        "\n",
        "These modules can be composed and customized to build robust, explainable ML pipelines tailored to your use case.\n",
        "\n",
        "Let's have a look at an example that performs RAG with chain of thought."
      ],
      "metadata": {
        "id": "7L_PfqaCjyqn"
      },
      "id": "7L_PfqaCjyqn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with Chain of Thought\n",
        "\n",
        "**Exercise:** Implement a simple RAG (Retrieval-Augmented Generation) example using ChainOfThought.\n",
        "\n",
        "- Define a function to search Wikipedia (use dspy.ColBERTv2)\n",
        "- Create a ChainOfThought module\n",
        "- Use it to answer a question\n",
        "\n",
        "Try to write the code yourself before revealing the solution!\n",
        "\n",
        "Instructions:\n",
        "- Write a function that retrieves a query as follows:\n",
        "```python\n",
        "results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
        "```\n",
        "\n",
        "Note! Some fellows have reported that they cannot get the above ColBERTv2 function to work.\n",
        "E.g. it could return a dummy list as follows to answer the question below:\n",
        "```python\n",
        " [{'text': 'Kinnairdy Castle',\n",
        "             'rank': 1,\n",
        "             'prob':0.7},\n",
        "             {'text': 'David Castle',\n",
        "             'rank': 2,\n",
        "             'prob':0.2},\n",
        "             {'text': 'Sir Gregory Castle',\n",
        "             'rank': 3,\n",
        "             'prob':0.1}\n",
        "             ]\n",
        "```\n",
        "\n",
        "Return the \"text\" part of those results to the chain of thought module.\n",
        "- Define a chain of thought module as follows: rag = dspy.ChainOfThought(\"context, question -> response\")\n",
        "This means you will take in context and question as inputs, and response will be the output.\n",
        "- Then call the rag function with the relevant inputs."
      ],
      "metadata": {
        "id": "xdYVa_pbr4-K"
      },
      "id": "xdYVa_pbr4-K"
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of how to query. Note, if running on your local machine, turn off Zscaler to use this functionality.\n",
        "query = \"What is the capital of France?\"\n",
        "results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYBYOeAxsR8P",
        "outputId": "13f1efd9-c534-44dc-d021-5235bd326a62"
      },
      "id": "TYBYOeAxsR8P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Paris (disambiguation) | Paris is the largest city and capital of France.',\n",
              "  'pid': 1578992,\n",
              "  'rank': 1,\n",
              "  'score': 26.49927520751953,\n",
              "  'prob': 0.5327062013039583,\n",
              "  'long_text': 'Paris (disambiguation) | Paris is the largest city and capital of France.'},\n",
              " {'text': 'Paris | Paris (] ) is the capital and most populous city of France, with an administrative-limits area of 105 km2 and a 2015 population of 2,229,621. The city is a commune and department, and the capital-heart of the 12,012 km2 Île-de-France \"region\" (colloquially known as the \\'Paris Region\\'), whose 12,142,802 2016 population represents roughly 18 percent of the population of France. By the 17th century, Paris had become one of Europe\\'s major centres of finance, commerce, fashion, science, and the arts, a position that it retains still today. The Paris Region had a GDP of €649.6 billion (US $763.4 billion) in 2014, accounting for 30.4 percent of the GDP of France. According to official estimates, in 2013-14 the Paris Region had the third-highest GDP in the world and the largest regional GDP in the EU.',\n",
              "  'pid': 3193249,\n",
              "  'rank': 2,\n",
              "  'score': 25.712459564208984,\n",
              "  'prob': 0.24253703292565776,\n",
              "  'long_text': 'Paris | Paris (] ) is the capital and most populous city of France, with an administrative-limits area of 105 km2 and a 2015 population of 2,229,621. The city is a commune and department, and the capital-heart of the 12,012 km2 Île-de-France \"region\" (colloquially known as the \\'Paris Region\\'), whose 12,142,802 2016 population represents roughly 18 percent of the population of France. By the 17th century, Paris had become one of Europe\\'s major centres of finance, commerce, fashion, science, and the arts, a position that it retains still today. The Paris Region had a GDP of €649.6 billion (US $763.4 billion) in 2014, accounting for 30.4 percent of the GDP of France. According to official estimates, in 2013-14 the Paris Region had the third-highest GDP in the world and the largest regional GDP in the EU.'},\n",
              " {'text': \"Administration of Paris | As the capital of France, Paris is the seat of France's national government. For the executive, the two chief officers each have their own official residences, which also serve as their offices. The President of France resides at the Élysée Palace in the 8th arrondissement, while the Prime Minister's seat is at the Hôtel Matignon in the 7th arrondissement. Government ministries are located in various parts of the city; many are located in the 7th arrondissement, near the Matignon.\",\n",
              "  'pid': 357013,\n",
              "  'rank': 3,\n",
              "  'score': 25.636323928833008,\n",
              "  'prob': 0.22475676577038406,\n",
              "  'long_text': \"Administration of Paris | As the capital of France, Paris is the seat of France's national government. For the executive, the two chief officers each have their own official residences, which also serve as their offices. The President of France resides at the Élysée Palace in the 8th arrondissement, while the Prime Minister's seat is at the Hôtel Matignon in the 7th arrondissement. Government ministries are located in various parts of the city; many are located in the 7th arrondissement, near the Matignon.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a simple RAG example\n",
        "def search_wikipedia(query:str) -> list[str]:\n",
        "    return \"TODO\"\n",
        "\n",
        "rag = None # TODO: Implement a simple RAG example\n",
        "question = \"What's the name of the castle that David Gregory inherited?\"\n",
        "\n",
        "#TODO: call the rag function you defined above with the context and question"
      ],
      "metadata": {
        "id": "EkKAiMmsshej"
      },
      "id": "EkKAiMmsshej",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "def search_wikipedia(query: str) -> list[str]:\n",
        "    results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
        "    return [x[\"text\"] for x in results]\n",
        "rag = dspy.ChainOfThought(\"context, question -> response\")\n",
        "question = \"What's the name of the castle that David Gregory inherited?\"\n",
        "rag(context=search_wikipedia(question), question=question)\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "OgBEiYsHsh_6"
      },
      "id": "OgBEiYsHsh_6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting History and Underlying Process in DSPy\n",
        "\n",
        "DSPy provides tools to inspect the reasoning, intermediate steps, and history of module executions. This is useful for debugging, understanding model decisions, and improving transparency.\n",
        "\n",
        "- Each module (such as Predict, ChainOfThought, or ReAct) can log its reasoning steps, inputs, outputs, and tool usage.\n",
        "- You can access the history and trace of a module run to see how the LM arrived at its answer.\n",
        "- For example, after running a module, you can inspect its `.history` attributes to view the sequence of actions and decisions.\n",
        "\n",
        "This inspection capability helps you debug pipelines, audit model behavior, and refine prompts or signatures for better results."
      ],
      "metadata": {
        "id": "Jc3bisfRtFHZ"
      },
      "id": "Jc3bisfRtFHZ"
    },
    {
      "cell_type": "code",
      "source": [
        "rag.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "YUNWb4l0tMRH",
        "outputId": "788d7464-c632-4908-a341-28f63b660946"
      },
      "id": "YUNWb4l0tMRH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'history'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-229881375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using dspy.inspect_history() to Analyze Pipeline Execution\n",
        "\n",
        "The function `dspy.inspect_history()` allows you to review the complete execution history of DSPy modules and agents within your pipeline. This is especially useful for debugging, auditing, and understanding how predictions and decisions were made.\n",
        "\n",
        "When you call `dspy.inspect_history()`, you can expect to find:\n",
        "- A chronological log of all module and agent invocations\n",
        "- The inputs and outputs for each step in the form of user message and response\n",
        "- Reasoning steps, intermediate variables, and tool usage\n",
        "- Any errors or exceptions encountered during execution\n",
        "- Metadata such as time stamps and external API/tool calls\n",
        "\n",
        "This detailed trace helps you identify bottlenecks, verify correctness, and improve transparency in your ML workflows. It is an essential tool for iterative development and for building explainable AI systems with DSPy."
      ],
      "metadata": {
        "id": "rCxMUHr2tRp0"
      },
      "id": "rCxMUHr2tRp0"
    },
    {
      "cell_type": "code",
      "source": [
        "#more generally you can inspect the dspy history to understand\n",
        "#notice how the context contains the search results\n",
        "dspy.inspect_history()"
      ],
      "metadata": {
        "id": "bucAQ3N9tS3c"
      },
      "id": "bucAQ3N9tS3c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}