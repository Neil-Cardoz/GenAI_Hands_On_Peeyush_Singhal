{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/GenAI_Hands_On/blob/main/Intro_DSPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e1de941",
      "metadata": {
        "id": "5e1de941"
      },
      "source": [
        "\n",
        "# Introduction to DSPy\n",
        "This interactive notebook is meant as a **learning guide** on how to use DSPy.\n",
        "\n",
        "You will be prompted to fill in key code sections yourself. For each exercise, try to write the code before revealing the solution!\n",
        "\n",
        "---\n",
        "\n",
        "You will learn about:\n",
        "- The role of language models (LMs) / GenAI\n",
        "- How DSPy Signatures and Modules are used\n",
        "\n",
        "In the following sections, we will explore these concepts with code examples and explanations.\n",
        "\n",
        "DSPy is designed for building **modular, explainable, and robust** AI pipelines that combine language models, structured reasoning, and tool integration. For packaging classification, this brings several advantages.\n",
        "\n",
        "The main benefit of DSPy is that it is a **declarative language**. The idea is to take away the manual prompt optimisation part and change it into a requirement formulation/programming task. This approach allows automatic prompt optimization methods.\n",
        "\n",
        "DSPy makes it straightforward to improve your pipelines by:\n",
        "\n",
        "- **Learnable Parameters:** DSPy modules can include learnable parameters (such as prompt templates or tool selection strategies) that can be automatically tuned for better performance.\n",
        "- **Automated Tuning:** DSPy provides optimizers (like `dspy.optimize`) that can automatically search for the best configuration of your pipeline, using labeled data or feedback signals.\n",
        "- **End-to-End Pipeline Optimization:** You can optimize not just individual prompts, but entire pipelines—including tool usage, reasoning steps, and output normalization—without rewriting your code.\n",
        "- **Rapid Experimentation:** Because DSPy modules are composable and declarative, you can quickly try different optimization strategies and compare results, accelerating development cycles.\n",
        "\n",
        "This means you can move from prototyping to production-ready, high-performing pipelines much faster, with less manual prompt engineering and more robust, data-driven improvements. The topic of optimization will be out of scope of this introductory notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7014a4cb",
      "metadata": {
        "id": "7014a4cb"
      },
      "source": [
        "\n",
        "## 0) Environment Setup\n",
        "\n",
        "This workshop uses:\n",
        "- Python 3.10+\n",
        "- `dspy` (or `dspy-ai`) for programmable, optimizable LLM pipelines\n",
        "- `pandas` for data handling\n",
        "- `rapidfuzz` for string similarity\n",
        "- An LLM provider (Gemini or OpenAI or compatible).\n",
        "\n",
        "> If you don't have Internet in your environment, skip installs and read through the code; it will still serve as a template.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your environment allows, uncomment to install.\n",
        "!pip install --quiet dspy-ai rapidfuzz pandas python-dotenv\n",
        "# For Google API:\n",
        "!pip install --quiet google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMoKJUEbudNI",
        "outputId": "ea897405-9874-4092-8a73-5995fae3eacc"
      },
      "id": "gMoKJUEbudNI",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "tCBHGYLiu0Iu"
      },
      "id": "tCBHGYLiu0Iu",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete. Data directory:\", DATA_DIR.resolve())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOGXkZhu51I",
        "outputId": "8bfc731c-1a85-4a05-de38-051b7a27d05f"
      },
      "id": "PXOGXkZhu51I",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Data directory: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee5191a",
      "metadata": {
        "id": "3ee5191a"
      },
      "source": [
        "\n",
        "### 0.1) Configure Model / API Keys\n",
        "\n",
        "You can use **Google Gemini** or **OpenAI** or any provider supported by DSPy\n",
        "Set the env var(s) appropriately, then initialize the DSPy model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "892d62ab",
      "metadata": {
        "id": "892d62ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dspy\n",
        "\n",
        "# Configure API Key\n",
        "GEMINI_API_KEY = \"AIzaSyAh0Kp5YuOCTmc5qKNo0R5cWzWWGe8x_OQ\"\n",
        "GEMINI_MODEL = \"gemini/gemini-2.5-flash\" # gemini-2.0-flash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa415e52",
      "metadata": {
        "id": "fa415e52"
      },
      "source": [
        "\n",
        "### 0.2) Initialize DSPy with your chosen language model\n",
        "\n",
        "Let's set up a simple LLM in DSPy. **Fill in the code cell below to:**\n",
        "- Import dspy\n",
        "- Set up a language model (you can use a placeholder for the API key)\n",
        "- Configure dspy to use this LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a30d9b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a30d9b01",
        "outputId": "8558744b-8669-42a4-9fc5-9943af0085f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy initialized with model: gemini/gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "# If DSPy is installed, this will work. Otherwise, treat as reference code.\n",
        "try:\n",
        "    import dspy\n",
        "    # Initialize a Gemini-based LM for DSPy (e.g., Gemini-2.5-flash)\n",
        "    llm = dspy.LM(\n",
        "        model= GEMINI_MODEL,\n",
        "        api_key=GEMINI_API_KEY\n",
        "    )\n",
        "    dspy.settings.configure(lm=llm)\n",
        "    print(\"DSPy initialized with model:\", GEMINI_MODEL)\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available or failed to initialize:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Calling the LLM\n",
        "\n",
        "Write a code cell to call the LLM with a simple prompt, e.g., 'Say this is a test!'.\n",
        "\n",
        "Note! If this does not work, most likely something is wrong with the setup of your LLM."
      ],
      "metadata": {
        "id": "t-jLqUtWhBSu"
      },
      "id": "t-jLqUtWhBSu"
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"Say: this is a test!\", temperature=0.7)  # => ['This is a test!']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq2FVJ-lg6fA",
        "outputId": "1882bad8-2c65-4842-c3f4-46f6be7e4c7e"
      },
      "id": "cq2FVJ-lg6fA",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is a test!']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the traditional role format: messages=\n",
        "[{\"role\": \"user\", \"content\": \"Say this is not a test!\"}]\n",
        "Try it here."
      ],
      "metadata": {
        "id": "i2mlWdwihZdv"
      },
      "id": "i2mlWdwihZdv"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Call the LLM with the messages format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd4FIxKShc7x",
        "outputId": "2880b899-cf87-4588-c15f-5b7ddc5fc45f"
      },
      "id": "hd4FIxKShc7x",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is not a test!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "llm(messages=[{\"role\": \"user\", \"content\": \"Say this is not a test!\"}])  # => ['This is not a test!']\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "qujfzRI9hgkx"
      },
      "id": "qujfzRI9hgkx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSPy Signatures and Modules\n",
        "\n",
        "**Exercise:** Define a simple DSPy signature for sentiment classification.\n",
        "\n",
        "- Create a class `Classify` inheriting from `dspy.Signature`\n",
        "- Add input and output fields for sentence, sentiment, and confidence\n",
        "- Instantiate a Predict module and use it on a sample sentence"
      ],
      "metadata": {
        "id": "coTovYW-h74_"
      },
      "id": "coTovYW-h74_"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "class Classify(dspy.Signature):\n",
        "    \"\"\"Classify sentiment of a given sentence.\"\"\"\n",
        "\n",
        "    sentence: str = dspy.InputField()\n",
        "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = dspy.OutputField()\n",
        "    confidence: float = dspy.OutputField()\n",
        "\n",
        "classify = dspy.Predict(Classify)\n",
        "classify(sentence=\"This book was super fun to read, though not the last chapter.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQEI8q9GiAVz",
        "outputId": "8a74a489-fc5e-493f-b39d-59f8c81fbd4d"
      },
      "id": "bQEI8q9GiAVz",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    sentiment='positive',\n",
              "    confidence=0.75\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBlBbevXin_B",
        "outputId": "d43d7a1e-3095-4570-e4a4-66e776bea04a"
      },
      "id": "dBlBbevXin_B",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': None,\n",
              "  'messages': [{'role': 'system',\n",
              "    'content': \"Your input fields are:\\n1. `sentence` (str):\\nYour output fields are:\\n1. `sentiment` (Literal['positive', 'negative', 'neutral']): \\n2. `confidence` (float):\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## sentence ## ]]\\n{sentence}\\n\\n[[ ## sentiment ## ]]\\n{sentiment}        # note: the value you produce must exactly match (no extra characters) one of: positive; negative; neutral\\n\\n[[ ## confidence ## ]]\\n{confidence}        # note: the value you produce must be a single float value\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Classify sentiment of a given sentence.\"},\n",
              "   {'role': 'user',\n",
              "    'content': \"[[ ## sentence ## ]]\\nThis book was super fun to read, though not the last chapter.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## sentiment ## ]]` (must be formatted as a valid Python Literal['positive', 'negative', 'neutral']), then `[[ ## confidence ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
              "  'kwargs': {},\n",
              "  'response': ModelResponse(id='Am2saLbkJ7Cc-8YPw6CtwAs', created=1756130558, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## sentiment ## ]]\\npositive\\n\\n[[ ## confidence ## ]]\\n0.75\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=508, prompt_tokens=251, total_tokens=759, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=484, rejected_prediction_tokens=None, text_tokens=24), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=251, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[], cache_hit=None),\n",
              "  'outputs': ['[[ ## sentiment ## ]]\\npositive\\n\\n[[ ## confidence ## ]]\\n0.75\\n\\n[[ ## completed ## ]]'],\n",
              "  'usage': {'completion_tokens': 508,\n",
              "   'prompt_tokens': 251,\n",
              "   'total_tokens': 759,\n",
              "   'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=484, rejected_prediction_tokens=None, text_tokens=24),\n",
              "   'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=251, image_tokens=None)},\n",
              "  'cost': 0.0013453,\n",
              "  'timestamp': '2025-08-25T14:02:42.665811',\n",
              "  'uuid': 'ff749fda-f705-4fbe-be9e-1f440b9d3601',\n",
              "  'model': 'gemini/gemini-2.5-flash',\n",
              "  'response_model': 'gemini-2.5-flash',\n",
              "  'model_type': 'chat'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.inspect_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MRql2tpief4",
        "outputId": "84367659-d960-4292-90ba-fe7350168189"
      },
      "id": "_MRql2tpief4",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-08-25T14:02:42.665811]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `sentence` (str):\n",
            "Your output fields are:\n",
            "1. `sentiment` (Literal['positive', 'negative', 'neutral']): \n",
            "2. `confidence` (float):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## sentence ## ]]\n",
            "{sentence}\n",
            "\n",
            "[[ ## sentiment ## ]]\n",
            "{sentiment}        # note: the value you produce must exactly match (no extra characters) one of: positive; negative; neutral\n",
            "\n",
            "[[ ## confidence ## ]]\n",
            "{confidence}        # note: the value you produce must be a single float value\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Classify sentiment of a given sentence.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## sentence ## ]]\n",
            "This book was super fun to read, though not the last chapter.\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## sentiment ## ]]` (must be formatted as a valid Python Literal['positive', 'negative', 'neutral']), then `[[ ## confidence ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## sentiment ## ]]\n",
            "positive\n",
            "\n",
            "[[ ## confidence ## ]]\n",
            "0.75\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd43331",
      "metadata": {
        "id": "5dd43331"
      },
      "source": [
        "\n",
        "---\n",
        "## 1) Warm-Up (GenAI Basics): Product Description Generator (FMCG)\n",
        "\n",
        "**Goal:** See how style, tone, and temperature affect outputs.\n",
        "\n",
        "**Task:** Given a product name & features, generate a short marketing description in 3 tones.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Different Tones\n",
        "product = \"Sunburst Orange Juice\"\n",
        "tones = [\"Formal\", \"Casual\", \"Punchy / Ad-like\"]"
      ],
      "metadata": {
        "id": "2mby186jxOkA"
      },
      "id": "2mby186jxOkA",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: write a DSPy predict function that has input of tone and product and outputs product description"
      ],
      "metadata": {
        "id": "OTbJ9U4Bj7jP"
      },
      "id": "OTbJ9U4Bj7jP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "# ✅ Define signature\n",
        "class ProductDescription(dspy.Signature):\n",
        "    \"\"\"Generate a product description given tone and product.\"\"\"\n",
        "    tone = dspy.InputField()\n",
        "    product = dspy.InputField()\n",
        "    description = dspy.OutputField()\n",
        "\n",
        "# Predict module\n",
        "gen = dspy.Predict(ProductDescription)\n",
        "\n",
        "for tone in tones:\n",
        "    result = gen(tone=tone, product=product)\n",
        "    print(f\"\\n--- {tone} ---\\n{result.description}\")\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "0GXsaLYhj6zt"
      },
      "id": "0GXsaLYhj6zt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***No writing prompts***"
      ],
      "metadata": {
        "id": "Tz-ZS2dL0QA4"
      },
      "id": "Tz-ZS2dL0QA4"
    },
    {
      "cell_type": "code",
      "source": [
        "# gen.inspect_history()"
      ],
      "metadata": {
        "id": "ciLCW4m70TYE"
      },
      "id": "ciLCW4m70TYE",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of DSPy Modules: Predict, ChainOfThought, and ReAct\n",
        "\n",
        "DSPy provides several module types to structure and control how language models solve tasks.\n",
        "What do you think Predict, ChainofThought and ReAct are used for?\n",
        "\n",
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "**dspy.Predict**:\n",
        "- The simplest module, used for direct prediction tasks.\n",
        "- Takes a signature and uses the LM to generate outputs based on the defined input/output fields.\n",
        "- Example: Sentiment classification, Product Description.\n",
        "\n",
        "**dspy.ChainOfThought**:\n",
        "- Encourages the LM to reason step-by-step before a final answer.\n",
        "- Useful for tasks that benefit from intermediate reasoning, such as complex classification or multi-step decision making.\n",
        "- Example: Explaining why a packaging type was chosen before giving the answer.\n",
        "\n",
        "**dspy.ReAct**:\n",
        "- This is essentially a react agent, that can interact with tools or external APIs during its reasoning process.\n",
        "- Useful for tasks that require both thought and action, such as searching for information or calling functions.\n",
        "- Example: The LM may search for product images, then download and view them, before deciding whether the image is appropriate.\n",
        "\n",
        "These modules can be composed and customized to build robust, explainable ML pipelines tailored to your use case.\n",
        "\n",
        "Let's have a look at an example that performs RAG with chain of thought."
      ],
      "metadata": {
        "id": "7L_PfqaCjyqn"
      },
      "id": "7L_PfqaCjyqn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with Chain of Thought\n",
        "\n",
        "**Exercise:** Implement a simple RAG (Retrieval-Augmented Generation) example using ChainOfThought.\n",
        "\n",
        "- Define a function to search Wikipedia (use dspy.ColBERTv2)\n",
        "- Create a ChainOfThought module\n",
        "- Use it to answer a question\n",
        "\n",
        "Try to write the code yourself before revealing the solution!\n",
        "\n",
        "Instructions:\n",
        "- Write a function that retrieves a query as follows:\n",
        "```python\n",
        "results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
        "```\n",
        "\n",
        "Note! Some fellows have reported that they cannot get the above ColBERTv2 function to work.\n",
        "E.g. it could return a dummy list as follows to answer the question below:\n",
        "```python\n",
        " [{'text': 'Kinnairdy Castle',\n",
        "             'rank': 1,\n",
        "             'prob':0.7},\n",
        "             {'text': 'David Castle',\n",
        "             'rank': 2,\n",
        "             'prob':0.2},\n",
        "             {'text': 'Sir Gregory Castle',\n",
        "             'rank': 3,\n",
        "             'prob':0.1}\n",
        "             ]\n",
        "```\n",
        "\n",
        "Return the \"text\" part of those results to the chain of thought module.\n",
        "- Define a chain of thought module as follows: rag = dspy.ChainOfThought(\"context, question -> response\")\n",
        "This means you will take in context and question as inputs, and response will be the output.\n",
        "- Then call the rag function with the relevant inputs."
      ],
      "metadata": {
        "id": "xdYVa_pbr4-K"
      },
      "id": "xdYVa_pbr4-K"
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of how to query. Note, if running on your local machine, turn off Zscaler to use this functionality.\n",
        "query = \"What is the capital of France?\"\n",
        "results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYBYOeAxsR8P",
        "outputId": "15f3bf13-fb9a-4df8-f0e6-1db6c18046be"
      },
      "id": "TYBYOeAxsR8P",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Paris (disambiguation) | Paris is the largest city and capital of France.',\n",
              "  'pid': 1578992,\n",
              "  'rank': 1,\n",
              "  'score': 26.49927520751953,\n",
              "  'prob': 0.5327062013039583,\n",
              "  'long_text': 'Paris (disambiguation) | Paris is the largest city and capital of France.'},\n",
              " {'text': 'Paris | Paris (] ) is the capital and most populous city of France, with an administrative-limits area of 105 km2 and a 2015 population of 2,229,621. The city is a commune and department, and the capital-heart of the 12,012 km2 Île-de-France \"region\" (colloquially known as the \\'Paris Region\\'), whose 12,142,802 2016 population represents roughly 18 percent of the population of France. By the 17th century, Paris had become one of Europe\\'s major centres of finance, commerce, fashion, science, and the arts, a position that it retains still today. The Paris Region had a GDP of €649.6 billion (US $763.4 billion) in 2014, accounting for 30.4 percent of the GDP of France. According to official estimates, in 2013-14 the Paris Region had the third-highest GDP in the world and the largest regional GDP in the EU.',\n",
              "  'pid': 3193249,\n",
              "  'rank': 2,\n",
              "  'score': 25.712459564208984,\n",
              "  'prob': 0.24253703292565776,\n",
              "  'long_text': 'Paris | Paris (] ) is the capital and most populous city of France, with an administrative-limits area of 105 km2 and a 2015 population of 2,229,621. The city is a commune and department, and the capital-heart of the 12,012 km2 Île-de-France \"region\" (colloquially known as the \\'Paris Region\\'), whose 12,142,802 2016 population represents roughly 18 percent of the population of France. By the 17th century, Paris had become one of Europe\\'s major centres of finance, commerce, fashion, science, and the arts, a position that it retains still today. The Paris Region had a GDP of €649.6 billion (US $763.4 billion) in 2014, accounting for 30.4 percent of the GDP of France. According to official estimates, in 2013-14 the Paris Region had the third-highest GDP in the world and the largest regional GDP in the EU.'},\n",
              " {'text': \"Administration of Paris | As the capital of France, Paris is the seat of France's national government. For the executive, the two chief officers each have their own official residences, which also serve as their offices. The President of France resides at the Élysée Palace in the 8th arrondissement, while the Prime Minister's seat is at the Hôtel Matignon in the 7th arrondissement. Government ministries are located in various parts of the city; many are located in the 7th arrondissement, near the Matignon.\",\n",
              "  'pid': 357013,\n",
              "  'rank': 3,\n",
              "  'score': 25.636323928833008,\n",
              "  'prob': 0.22475676577038406,\n",
              "  'long_text': \"Administration of Paris | As the capital of France, Paris is the seat of France's national government. For the executive, the two chief officers each have their own official residences, which also serve as their offices. The President of France resides at the Élysée Palace in the 8th arrondissement, while the Prime Minister's seat is at the Hôtel Matignon in the 7th arrondissement. Government ministries are located in various parts of the city; many are located in the 7th arrondissement, near the Matignon.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a simple RAG example\n",
        "def search_wikipedia(query:str) -> list[str]:\n",
        "    return \"TODO\"\n",
        "\n",
        "rag = None # TODO: Implement a simple RAG example\n",
        "question = \"What's the name of the castle that David Gregory inherited?\"\n",
        "\n",
        "#TODO: call the rag function you defined above with the context and question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkKAiMmsshej",
        "outputId": "9551fa5c-e811-4bec-b323-40defa14ff28"
      },
      "id": "EkKAiMmsshej",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    reasoning='The question asks for the name of the castle inherited by David Gregory. I will locate the text discussing David Gregory and find the sentence that mentions him inheriting a castle. Paragraph [1] states, \"He inherited Kinnairdy Castle in 1664.\"',\n",
              "    response='Kinnairdy Castle'\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>Click to show solution</summary>\n",
        "\n",
        "```python\n",
        "def search_wikipedia(query: str) -> list[str]:\n",
        "    results = dspy.ColBERTv2(url=\"http://20.102.90.50:2017/wiki17_abstracts\")(query, k=3)\n",
        "    return [x[\"text\"] for x in results]\n",
        "rag = dspy.ChainOfThought(\"context, question -> response\")\n",
        "question = \"What's the name of the castle that David Gregory inherited?\"\n",
        "rag(context=search_wikipedia(question), question=question)\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "OgBEiYsHsh_6"
      },
      "id": "OgBEiYsHsh_6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting History and Underlying Process in DSPy\n",
        "\n",
        "DSPy provides tools to inspect the reasoning, intermediate steps, and history of module executions. This is useful for debugging, understanding model decisions, and improving transparency.\n",
        "\n",
        "- Each module (such as Predict, ChainOfThought, or ReAct) can log its reasoning steps, inputs, outputs, and tool usage.\n",
        "- You can access the history and trace of a module run to see how the LM arrived at its answer.\n",
        "- For example, after running a module, you can inspect its `.history` attributes to view the sequence of actions and decisions.\n",
        "\n",
        "This inspection capability helps you debug pipelines, audit model behavior, and refine prompts or signatures for better results."
      ],
      "metadata": {
        "id": "Jc3bisfRtFHZ"
      },
      "id": "Jc3bisfRtFHZ"
    },
    {
      "cell_type": "code",
      "source": [
        "rag.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUNWb4l0tMRH",
        "outputId": "689547bd-337b-44e8-cbfb-7db0904d5862"
      },
      "id": "YUNWb4l0tMRH",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': None,\n",
              "  'messages': [{'role': 'system',\n",
              "    'content': 'Your input fields are:\\n1. `context` (str): \\n2. `question` (str):\\nYour output fields are:\\n1. `reasoning` (str): \\n2. `response` (str):\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## context ## ]]\\n{context}\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Given the fields `context`, `question`, produce the fields `response`.'},\n",
              "   {'role': 'user',\n",
              "    'content': '[[ ## context ## ]]\\n[1] «David Gregory (physician) | David Gregory (20 December 1625 – 1720) was a Scottish physician and inventor. His surname is sometimes spelt as Gregorie, the original Scottish spelling. He inherited Kinnairdy Castle in 1664. Three of his twenty-nine children became mathematics professors. He is credited with inventing a military cannon that Isaac Newton described as \"being destructive to the human species\". Copies and details of the model no longer exist. Gregory\\'s use of a barometer to predict farming-related weather conditions led him to be accused of witchcraft by Presbyterian ministers from Aberdeen, although he was never convicted.»\\n[2] «Gregory Tarchaneiotes | Gregory Tarchaneiotes (Greek: Γρηγόριος Ταρχανειώτης , Italian: \"Gregorio Tracanioto\" or \"Tracamoto\" ) was a \"protospatharius\" and the long-reigning catepan of Italy from 998 to 1006. In December 999, and again on February 2, 1002, he reinstituted and confirmed the possessions of the abbey and monks of Monte Cassino in Ascoli. In 1004, he fortified and expanded the castle of Dragonara on the Fortore. He gave it three circular towers and one square one. He also strengthened Lucera.»\\n[3] «Gregory of Gaeta | Gregory was the Duke of Gaeta from 963 until his death. He was the second son of Docibilis II of Gaeta and his wife Orania. He succeeded his brother John II, who had left only daughters. Gregory rapidly depleted the \"publicum\" (public land) of the Duchy of Gaeta by doling it out to family members as grants. Gregory disappears from the records in 964 and was succeeded by his younger brother Marinus of Fondi over the heads of his three sons. It is possible that there was an internal power struggle between factions of the Docibilan family and that Gregory was forced out. On the other hand, perhaps he died and his sons fought a losing battle for their inheritance to Gaeta.»\\n\\n[[ ## question ## ]]\\nWhat\\'s the name of the castle that David Gregory inherited?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}],\n",
              "  'kwargs': {},\n",
              "  'response': ModelResponse(id='ZnesaLayBcOY-8YPr8qu4QI', created=1756133220, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe question asks for the name of the castle inherited by David Gregory. I will locate the text discussing David Gregory and find the sentence that mentions him inheriting a castle. Paragraph [1] states, \"He inherited Kinnairdy Castle in 1664.\"\\n\\n[[ ## response ## ]]\\nKinnairdy Castle\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage={}, vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[], cache_hit=None),\n",
              "  'outputs': ['[[ ## reasoning ## ]]\\nThe question asks for the name of the castle inherited by David Gregory. I will locate the text discussing David Gregory and find the sentence that mentions him inheriting a castle. Paragraph [1] states, \"He inherited Kinnairdy Castle in 1664.\"\\n\\n[[ ## response ## ]]\\nKinnairdy Castle\\n\\n[[ ## completed ## ]]'],\n",
              "  'usage': {},\n",
              "  'cost': 0.000646,\n",
              "  'timestamp': '2025-08-25T14:48:30.762374',\n",
              "  'uuid': '7d4c0462-401a-4e54-816c-ba0f9dc3e9f7',\n",
              "  'model': 'gemini/gemini-2.5-flash',\n",
              "  'response_model': 'gemini-2.5-flash',\n",
              "  'model_type': 'chat'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using dspy.inspect_history() to Analyze Pipeline Execution\n",
        "\n",
        "The function `dspy.inspect_history()` allows you to review the complete execution history of DSPy modules and agents within your pipeline. This is especially useful for debugging, auditing, and understanding how predictions and decisions were made.\n",
        "\n",
        "When you call `dspy.inspect_history()`, you can expect to find:\n",
        "- A chronological log of all module and agent invocations\n",
        "- The inputs and outputs for each step in the form of user message and response\n",
        "- Reasoning steps, intermediate variables, and tool usage\n",
        "- Any errors or exceptions encountered during execution\n",
        "- Metadata such as time stamps and external API/tool calls\n",
        "\n",
        "This detailed trace helps you identify bottlenecks, verify correctness, and improve transparency in your ML workflows. It is an essential tool for iterative development and for building explainable AI systems with DSPy."
      ],
      "metadata": {
        "id": "rCxMUHr2tRp0"
      },
      "id": "rCxMUHr2tRp0"
    },
    {
      "cell_type": "code",
      "source": [
        "#more generally you can inspect the dspy history to understand\n",
        "#notice how the context contains the search results\n",
        "dspy.inspect_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bucAQ3N9tS3c",
        "outputId": "9b445c3e-5618-418b-f06f-c90b5048528a"
      },
      "id": "bucAQ3N9tS3c",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2025-08-25T14:48:30.762374]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `context` (str): \n",
            "2. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `response` (str):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## context ## ]]\n",
            "{context}\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## response ## ]]\n",
            "{response}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Given the fields `context`, `question`, produce the fields `response`.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## context ## ]]\n",
            "[1] «David Gregory (physician) | David Gregory (20 December 1625 – 1720) was a Scottish physician and inventor. His surname is sometimes spelt as Gregorie, the original Scottish spelling. He inherited Kinnairdy Castle in 1664. Three of his twenty-nine children became mathematics professors. He is credited with inventing a military cannon that Isaac Newton described as \"being destructive to the human species\". Copies and details of the model no longer exist. Gregory's use of a barometer to predict farming-related weather conditions led him to be accused of witchcraft by Presbyterian ministers from Aberdeen, although he was never convicted.»\n",
            "[2] «Gregory Tarchaneiotes | Gregory Tarchaneiotes (Greek: Γρηγόριος Ταρχανειώτης , Italian: \"Gregorio Tracanioto\" or \"Tracamoto\" ) was a \"protospatharius\" and the long-reigning catepan of Italy from 998 to 1006. In December 999, and again on February 2, 1002, he reinstituted and confirmed the possessions of the abbey and monks of Monte Cassino in Ascoli. In 1004, he fortified and expanded the castle of Dragonara on the Fortore. He gave it three circular towers and one square one. He also strengthened Lucera.»\n",
            "[3] «Gregory of Gaeta | Gregory was the Duke of Gaeta from 963 until his death. He was the second son of Docibilis II of Gaeta and his wife Orania. He succeeded his brother John II, who had left only daughters. Gregory rapidly depleted the \"publicum\" (public land) of the Duchy of Gaeta by doling it out to family members as grants. Gregory disappears from the records in 964 and was succeeded by his younger brother Marinus of Fondi over the heads of his three sons. It is possible that there was an internal power struggle between factions of the Docibilan family and that Gregory was forced out. On the other hand, perhaps he died and his sons fought a losing battle for their inheritance to Gaeta.»\n",
            "\n",
            "[[ ## question ## ]]\n",
            "What's the name of the castle that David Gregory inherited?\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "The question asks for the name of the castle inherited by David Gregory. I will locate the text discussing David Gregory and find the sentence that mentions him inheriting a castle. Paragraph [1] states, \"He inherited Kinnairdy Castle in 1664.\"\n",
            "\n",
            "[[ ## response ## ]]\n",
            "Kinnairdy Castle\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31d6e45",
      "metadata": {
        "id": "f31d6e45"
      },
      "source": [
        "\n",
        "---\n",
        "## 2) Maps Mini-Project: Street Name Normalization & Matching\n",
        "\n",
        "**Problem:** Real-world street names vary (`\"MG Road\"`, `\"M.G. Rd\"`, `\"Mahatma Gandhi Road\"`).  \n",
        "**Goal:** Normalize variants to a canonical form and match duplicates.\n",
        "\n",
        "We'll combine:\n",
        "- **LLM-based normalization** (expand abbreviations, fix casing, remove punctuation)\n",
        "- **String similarity** via `rapidfuzz` for robust matching\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca4285d",
      "metadata": {
        "id": "aca4285d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz, process\n",
        "\n",
        "# Sample data with variants\n",
        "streets = pd.DataFrame({\n",
        "    \"raw_street\": [\n",
        "        \"MG Road\", \"M.G. Rd\", \"Mahatma Gandhi Rd\", \"Mahatma Gandhi Road\",\n",
        "        \"St John's Rd\", \"Saint Johns Road\", \"St. John’s Rd\", \"St. John Road\",\n",
        "        \"Nehru Marg\", \"Jawaharlal Nehru Marg\", \"J L Nehru Marg\",\n",
        "        \"Ring Rd\", \"Outer Ring Road\", \"Outer Rng Rd\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "streets.to_csv(\"data/streets_raw.csv\", index=False)\n",
        "streets.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e043d0",
      "metadata": {
        "id": "e9e043d0"
      },
      "source": [
        "\n",
        "### 2.1) LLM Normalizer (DSPy)\n",
        "\n",
        "We'll create a simple **Signature** and **Predictor** that maps a raw street name → canonical normalized name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a8cfa54",
      "metadata": {
        "id": "2a8cfa54"
      },
      "outputs": [],
      "source": [
        "\n",
        "normalizer_spec = \"\"\"\n",
        "Given an Indian street name variant, return a clean, canonical, expanded form:\n",
        "- Expand common abbreviations (e.g., 'Rd' → 'Road', 'St' → 'Saint' when it's a person's name; else 'Street' if context suggests)\n",
        "- Remove unnecessary punctuation\n",
        "- Use Title Case\n",
        "- Prefer full names (e.g., 'MG' → 'Mahatma Gandhi' when unambiguous)\n",
        "Return only the normalized name, no extra text.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class NormalizeStreet(dspy.Signature):\n",
        "        raw_name = dspy.InputField()\n",
        "        normalized = dspy.OutputField(desc=\"normalized, canonical street name\")\n",
        "\n",
        "    normalize = dspy.Predict(NormalizeStreet)\n",
        "\n",
        "    def llm_normalize(name: str) -> str:\n",
        "        r = normalize(raw_name=f\"{name}\n",
        "\n",
        "Guidelines:\n",
        "{normalizer_spec}\")\n",
        "        return r.normalized.strip()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available; falling back to a rule-based normalizer:\", e)\n",
        "    import re\n",
        "\n",
        "    ABBR = {\n",
        "        r\"\\brd\\b\": \"Road\",\n",
        "        r\"\\brd.\\b\": \"Road\",\n",
        "        r\"\\bst\\b\": \"Street\",\n",
        "        r\"\\bst.\\b\": \"Street\",\n",
        "        r\"\\bmg\\b\": \"Mahatma Gandhi\",\n",
        "        r\"\\bjl\\b\": \"Jawaharlal\",\n",
        "        r\"\\bmarg\\b\": \"Marg\",\n",
        "        r\"\\brng\\b\": \"Ring\",\n",
        "    }\n",
        "    def rule_normalize(text: str) -> str:\n",
        "        t = text.lower()\n",
        "        for pat, rep in ABBR.items():\n",
        "            t = re.sub(pat, rep.lower(), t)\n",
        "        t = re.sub(r\"[.’']\", \"\", t)\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "        return t.title()\n",
        "\n",
        "    def llm_normalize(name: str) -> str:\n",
        "        return rule_normalize(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ddcc682",
      "metadata": {
        "id": "1ddcc682"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"data/streets_raw.csv\")\n",
        "df[\"normalized\"] = df[\"raw_street\"].apply(llm_normalize)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d284b300",
      "metadata": {
        "id": "d284b300"
      },
      "source": [
        "\n",
        "### 2.2) Fuzzy Matching to Group Duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c101ce",
      "metadata": {
        "id": "f0c101ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Group streets by similarity of their normalized form\n",
        "# We'll use a simple threshold; in production, tune per locale and evaluate with ground truth.\n",
        "threshold = 90\n",
        "\n",
        "unique_norms = df[\"normalized\"].unique().tolist()\n",
        "clusters = []\n",
        "visited = set()\n",
        "\n",
        "for i, s in enumerate(unique_norms):\n",
        "    if s in visited:\n",
        "        continue\n",
        "    visited.add(s)\n",
        "    # Find close matches\n",
        "    matches = process.extract(s, unique_norms, scorer=fuzz.token_sort_ratio, limit=None)\n",
        "    group = [m[0] for m in matches if m[1] >= threshold]\n",
        "    clusters.append(group)\n",
        "    visited.update(group)\n",
        "\n",
        "# Map each row to a cluster id\n",
        "cluster_map = {}\n",
        "for idx, group in enumerate(clusters):\n",
        "    for g in group:\n",
        "        cluster_map[g] = idx\n",
        "\n",
        "df[\"cluster_id\"] = df[\"normalized\"].map(cluster_map)\n",
        "df.sort_values([\"cluster_id\", \"normalized\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0a7da9",
      "metadata": {
        "id": "2c0a7da9"
      },
      "source": [
        "\n",
        "**Exercise:** Try changing the `threshold` to see how clusters merge/split.  \n",
        "**Discussion:** When to trust LLM normalization vs rules; human-in-the-loop QA for map data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15b9003d",
      "metadata": {
        "id": "15b9003d"
      },
      "source": [
        "\n",
        "---\n",
        "## 3) FMCG Mini-Project: Reviews → Insights → Actions\n",
        "\n",
        "**Goal:** Generate synthetic reviews for a new product, summarize themes, extract insights, and recommend actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3333d9",
      "metadata": {
        "id": "ba3333d9"
      },
      "outputs": [],
      "source": [
        "\n",
        "product = \"SunBurst Orange Juice\"\n",
        "aspects = [\"taste\", \"price\", \"packaging\", \"availability\", \"healthiness\"]\n",
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class ReviewSynth(dspy.Signature):\n",
        "        product = dspy.InputField()\n",
        "        aspects = dspy.InputField()\n",
        "        reviews = dspy.OutputField(desc=\"10 diverse, short customer reviews\")\n",
        "\n",
        "    synth = dspy.Predict(ReviewSynth)\n",
        "    reviews_text = synth(product=product, aspects=aspects).reviews\n",
        "except Exception:\n",
        "    # Fallback: sample static reviews\n",
        "    reviews_text = \"\"\"\n",
        "1) Great taste but a bit pricey.\n",
        "2) Love the no-sugar claim; feels healthy.\n",
        "3) Packaging leaks if kept sideways.\n",
        "4) Hard to find at my local store.\n",
        "5) Kids enjoy it; refreshing and pulpy.\n",
        "6) Price is okay during discounts.\n",
        "7) Wish there was a smaller pack size.\n",
        "8) Tastes natural, not too sweet.\n",
        "9) Outer packaging is attractive.\n",
        "10) Delivery took long; store was out of stock.\n",
        "\"\"\"\n",
        "\n",
        "print(reviews_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30d95f7",
      "metadata": {
        "id": "a30d95f7"
      },
      "source": [
        "\n",
        "### 3.1) Summarize & Extract Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91431ec",
      "metadata": {
        "id": "a91431ec"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class SummarizeReviews(dspy.Signature):\n",
        "        reviews = dspy.InputField()\n",
        "        summary = dspy.OutputField(desc=\"pros, cons, notable quotes\")\n",
        "\n",
        "    class ExtractInsights(dspy.Signature):\n",
        "        summary = dspy.InputField()\n",
        "        insights = dspy.OutputField(desc=\"3-5 crisp insights with evidence\")\n",
        "\n",
        "    summarize = dspy.ChainOfThought(SummarizeReviews)\n",
        "    extract = dspy.Predict(ExtractInsights)\n",
        "\n",
        "    summary = summarize(reviews=reviews_text).summary\n",
        "    insights = extract(summary=summary).insights\n",
        "\n",
        "    print(\"SUMMARY:\\n\", summary)\n",
        "    print(\"\\nINSIGHTS:\\n\", insights)\n",
        "\n",
        "except Exception:\n",
        "    print(\"DSPy not available; here is a template prompt you can run with your LLM:\")\n",
        "    print(\"\"\"\n",
        "Summarize the following reviews into pros, cons, and notable quotes. Then provide 3-5 crisp insights:\n",
        "\"\"\")\n",
        "    print(reviews_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94059490",
      "metadata": {
        "id": "94059490"
      },
      "source": [
        "\n",
        "---\n",
        "## 4) Agentic AI with DSPy: Compose a Pipeline\n",
        "\n",
        "We'll build a 3-stage pipeline:\n",
        "1. **Summarizer** – condense reviews/sales text\n",
        "2. **Insight Generator** – extract trends/causes\n",
        "3. **Recommender** – propose next actions (pricing, packaging, distribution, marketing)\n",
        "\n",
        "You'll see: how **modules** wrap LLM calls, how to **swap models**, and how to **optimize prompts**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f0f437",
      "metadata": {
        "id": "f0f0f437"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    class Summarizer(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            class Sig(dspy.Signature):\n",
        "                text = dspy.InputField()\n",
        "                summary = dspy.OutputField()\n",
        "            self.step = dspy.ChainOfThought(Sig)\n",
        "        def forward(self, text):\n",
        "            return self.step(text=text).summary\n",
        "\n",
        "    class InsightGen(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            class Sig(dspy.Signature):\n",
        "                summary = dspy.InputField()\n",
        "                insights = dspy.OutputField()\n",
        "            self.step = dspy.Predict(Sig)\n",
        "        def forward(self, summary):\n",
        "            return self.step(summary=summary).insights\n",
        "\n",
        "    class Recommender(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            class Sig(dspy.Signature):\n",
        "                insights = dspy.InputField()\n",
        "                actions = dspy.OutputField()\n",
        "            self.step = dspy.Predict(Sig)\n",
        "        def forward(self, insights):\n",
        "            return self.step(insights=insights).actions\n",
        "\n",
        "    class FMCGPipeline(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.summarizer = Summarizer()\n",
        "            self.insightgen = InsightGen()\n",
        "            self.recommender = Recommender()\n",
        "\n",
        "        def forward(self, text):\n",
        "            summary = self.summarizer(text=text)\n",
        "            insights = self.insightgen(summary=summary)\n",
        "            actions = self.recommender(insights=insights)\n",
        "            return dict(summary=summary, insights=insights, actions=actions)\n",
        "\n",
        "    pipeline = FMCGPipeline()\n",
        "\n",
        "    sample_text = reviews_text\n",
        "    result = pipeline(text=sample_text)\n",
        "    print(\"SUMMARY:\\n\", result[\"summary\"])\n",
        "    print(\"\\nINSIGHTS:\\n\", result[\"insights\"])\n",
        "    print(\"\\nACTIONS:\\n\", result[\"actions\"])\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"DSPy not available; here is the logical flow you can implement with any LLM:\")\n",
        "    print(\"1) Summarize -> 2) Extract Insights -> 3) Recommend Actions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93fe85a9",
      "metadata": {
        "id": "93fe85a9"
      },
      "source": [
        "\n",
        "### 4.1) (Optional) DSPy Optimization\n",
        "\n",
        "DSPy supports **teleprompter**-style optimization given labeled examples.  \n",
        "Below is a minimal sketch (fill `train_data` with (input, target) pairs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7e5e21",
      "metadata": {
        "id": "5b7e5e21"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import dspy\n",
        "\n",
        "    # Minimal demo dataset (toy). Replace with real (input, target) pairs.\n",
        "    train_data = [\n",
        "        dict(text=\"Pricey but delicious. Hard to find locally.\", target_actions=\"Run local availability campaign; limited-time discount\"),\n",
        "        dict(text=\"Leaky packaging. Love the no sugar.\", target_actions=\"Improve cap seal; emphasize health benefit in ads\"),\n",
        "    ]\n",
        "\n",
        "    class ActionsTeacher(dspy.Signature):\n",
        "        text = dspy.InputField()\n",
        "        actions = dspy.OutputField()\n",
        "\n",
        "    # A tiny trainer that pretends \"actions\" is the supervised target.\n",
        "    class TinyTrainer(dspy.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.pipeline = FMCGPipeline()\n",
        "        def forward(self, text):\n",
        "            out = self.pipeline(text=text)\n",
        "            return out[\"actions\"]\n",
        "\n",
        "    # In real usage, use dspy.teleprompt.BootstrapFewShot or similar.\n",
        "    # Here we simply run the pipeline on training data as illustration.\n",
        "    trainer = TinyTrainer()\n",
        "    for ex in train_data:\n",
        "        _ = trainer(text=ex[\"text\"])\n",
        "    print(\"Optimization sketch complete (replace with DSPy teleprompters in real training).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Skipping optimization sketch due to:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df2d0d5",
      "metadata": {
        "id": "9df2d0d5"
      },
      "source": [
        "\n",
        "---\n",
        "## 5) Stretch Goals\n",
        "- Add a **retrieval** step (RAG) for product manuals/FAQs before generating actions.\n",
        "- Use a **validator** module to check if actions are grounded in the summary.\n",
        "- For Maps: add **house-number parsing**, **localization**, and **confidence scoring**.\n",
        "- Log prompts/outputs and build a small **evaluation harness** with golden test cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53aa3cc5",
      "metadata": {
        "id": "53aa3cc5"
      },
      "source": [
        "\n",
        "---\n",
        "## 6) Troubleshooting\n",
        "\n",
        "- **No Internet?** Skip installs, read through code, and run later on a connected machine.\n",
        "- **API errors?** Check `OPENAI_API_KEY`, `OPENAI_BASE_URL`, and `OPENAI_MODEL` env vars.\n",
        "- **DSPy version mismatch?** Adjust the LM initialization to your version.\n",
        "- **String matching too strict?** Lower the threshold or use another scorer.\n",
        "- **Time check:** Generated on 2025-08-23 02:32:58.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}